{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343fa2b",
   "metadata": {},
   "source": [
    "# MAKI4U Jumpstart Notebook\n",
    "\n",
    "A Notebook for training new BERT models for MAKI4U (former CCAI)\\\n",
    "This is a refactored version of \"bert_train_classifier.ipynb\" from the\n",
    "BAS Jumpstart\\ and is meant as optimization and general clean up of that notebook\\\n",
    "It is possible to use this as notebook or directly as a script\n",
    "\n",
    "\n",
    "This notebook is organized in\n",
    "* [Configuration for Model and Logging](#config)\n",
    "* [Loading Dataset](#dataset)\n",
    "* [Model Definition](#model)\n",
    "* [Train Model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89bab5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e878f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c49b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "from IPython import get_ipython\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "import yaml\n",
    "from utils.bert_custom_trainer import TrainerLossNetwork, TrainerDiceLoss\n",
    "from utils.configuration import (\n",
    "    parse_arguments,\n",
    "    save_config,\n",
    "    yaml_dump_for_notebook,\n",
    "    isnotebook,\n",
    ")\n",
    "from utils.metrics import Metrics\n",
    "from utils import scorer\n",
    "from utils.BERT import BERT\n",
    "from utils.result_collector import ResultCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2e68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.9.0.dev0\")\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc053f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27732f39",
   "metadata": {},
   "source": [
    "## Configuration and Logging <a class=\"anchor\" id=\"config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef8a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra variable for processing big files in BERT\n",
    "# https://github.com/huggingface/datasets/issues/2181\n",
    "block_size_10MB = 10 << 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97513e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = yaml_dump_for_notebook(filepath='configs/hierarchical-baseline.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b616d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003 log\\hierarchical-classification\\roberta\\roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\n"
     ]
    }
   ],
   "source": [
    "filename, filepath = save_config(args_dict)\n",
    "print(filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feef3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"{filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadf037",
   "metadata": {},
   "source": [
    "## Loading Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "### TODO: Refactor this properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446f7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = str(Path(args_dict[\"data_folder\"]).joinpath(args_dict[\"data_file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d711ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c7cb2b5b9bd6dc9b\n",
      "Reusing dataset json (C:\\Users\\mandy\\.cache\\huggingface\\datasets\\json\\default-c7cb2b5b9bd6dc9b\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.02it/s]\n",
      "Using custom data configuration default-c7723c49fb6bcfac\n",
      "Reusing dataset json (C:\\Users\\mandy\\.cache\\huggingface\\datasets\\json\\default-c7723c49fb6bcfac\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.20it/s]\n"
     ]
    }
   ],
   "source": [
    "json_files_train = [json_files.replace(\".json\", \"\") + \"_train.json\"]\n",
    "json_files_test = [json_files.replace(\".json\", \"\") + \"_test.json\"]\n",
    "\n",
    "dataset_train = load_dataset(\n",
    "    \"json\", data_files=json_files_train, chunksize=block_size_10MB\n",
    ")[\"train\"]\n",
    "\n",
    "dataset_test = load_dataset(\n",
    "    \"json\", data_files=json_files_test, chunksize=block_size_10MB\n",
    ")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f40239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['task_type'] == 'flat-classification' or args_dict['task_type'] == 'NER':\n",
    "    if args_dict['data_lvl']:\n",
    "        dataset_train = dataset_train.remove_columns(\"label\")\n",
    "        dataset_test = dataset_test.remove_columns(\"label\")\n",
    "\n",
    "        dataset_train = dataset_train.rename_column(f\"lvl{args_dict['data_lvl']}\", \"label\")\n",
    "        dataset_test = dataset_test.rename_column(f\"lvl{args_dict['data_lvl']}\", \"label\")\n",
    "\n",
    "    dataset_train = dataset_train.class_encode_column(\"label\")\n",
    "    dataset_test = dataset_test.class_encode_column(\"label\")\n",
    "\n",
    "elif args_dict['task_type'] == 'hierarchical-classification':\n",
    "    dataset_train = dataset_train.remove_columns(\"label\")\n",
    "    dataset_test = dataset_test.remove_columns(\"label\")\n",
    "\n",
    "    dataset_train = dataset_train.rename_column(\"path_list\", \"label\")\n",
    "    dataset_test = dataset_test.rename_column(\"path_list\", \"label\")\n",
    "\n",
    "# removes unnecessary columns\n",
    "rmv_col = [col for col in dataset_train.column_names if col not in ['label', 'text']]\n",
    "dataset_train = dataset_train.remove_columns(rmv_col)\n",
    "dataset_test = dataset_test.remove_columns(rmv_col)\n",
    "\n",
    "# assert (\n",
    "#     set(dataset_train['label']) == set(dataset_test['label'])\n",
    "# ), \"Something went wrong, target_names of train and test should be the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008313ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/vocab.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\cd250a0641fc611df69ec5dab3cceeb2ebc108d714428e842a06b002b545c651.fb599d871d4c63be4332170842c6ee8c3af3aca52d1f7df2765501f9b7317dbd\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/merges.txt from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\a587d479394d6433863fe521ebeaaf1f25165f0b37028ff451678043890b0ad5.14a8d77447799383f7f1e013aac40728fbe7510b6625ece6697ef91bef188a5d\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args_dict[\"checkpoint_model_or_path\"], use_fast=True, model_max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e601a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mandy\\.cache\\huggingface\\datasets\\json\\default-c7cb2b5b9bd6dc9b\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-d9692072dba6c28b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\mandy\\.cache\\huggingface\\datasets\\json\\default-c7723c49fb6bcfac\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-529a8006b1c208d2.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "num_labels = len(np.unique(dataset_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bafccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\mandy\\.cache\\huggingface\\datasets\\json\\default-c7cb2b5b9bd6dc9b\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-c41adf3bce22dee1.arrow\n"
     ]
    }
   ],
   "source": [
    "# use shuffle to make sure the order of samples is randomized (deterministically)\n",
    "dataset_train = dataset_train.shuffle(seed=args_dict[\"random_seed\"])\n",
    "ds_train_testvalid = dataset_train.train_test_split(\n",
    "    test_size=(1 - args_dict[\"split_ratio_train\"])\n",
    ")\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train_testvalid['train'],\n",
    "        \"valid\": ds_train_testvalid['test'],\n",
    "        \"test\": dataset_test\n",
    "    }\n",
    ")\n",
    "\n",
    "if args_dict[\"oversampling\"]:\n",
    "    target_names = np.unique(dataset[\"test\"][\"label\"])\n",
    "    df_train = dataset[\"train\"].to_pandas()\n",
    "    min_samples = math.ceil(len(df_train) * args_dict[\"oversampling\"])\n",
    "    count_dict = dict(df_train[\"label\"].value_counts())\n",
    "    count_dict = {k: v for k, v in count_dict.items() if v < min_samples}\n",
    "\n",
    "    over_samples = []\n",
    "    for label_id, n_occurance in count_dict.items():\n",
    "        class_samples = df_train[df_train[\"label\"] == label_id]\n",
    "        additional_samples = class_samples.sample(\n",
    "            n=(min_samples - len(class_samples)), replace=True\n",
    "        )\n",
    "        over_samples.append(additional_samples)\n",
    "        print(\n",
    "            f\"\\nAdding {len(additional_samples)} samples for class {target_names[label_id]}\"\n",
    "        )\n",
    "\n",
    "    new_train = pd.concat([df_train, *over_samples])\n",
    "    dataset[\"train\"] = Dataset.from_pandas(new_train)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args_dict[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcc63e",
   "metadata": {},
   "source": [
    "## Model Definition <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9247cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/vocab.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\cd250a0641fc611df69ec5dab3cceeb2ebc108d714428e842a06b002b545c651.fb599d871d4c63be4332170842c6ee8c3af3aca52d1f7df2765501f9b7317dbd\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/merges.txt from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\a587d479394d6433863fe521ebeaaf1f25165f0b37028ff451678043890b0ad5.14a8d77447799383f7f1e013aac40728fbe7510b6625ece6697ef91bef188a5d\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/uklfr/gottbert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/uklfr/gottbert-base/resolve/main/config.json from cache at C:\\Users\\mandy/.cache\\huggingface\\transformers\\6f6c31413ac098863f9c968d7e4e88a5e15a8c979c1e5b182b13a53b18959607.258e9fc11a3defe5712b762f321a17f7912a44e419c7003ae492df2714adcca2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52009\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.encoder.layer.8.attention.self.query.weight: True\n",
      "model.encoder.layer.8.attention.self.query.bias: True\n",
      "model.encoder.layer.8.attention.self.key.weight: True\n",
      "model.encoder.layer.8.attention.self.key.bias: True\n",
      "model.encoder.layer.8.attention.self.value.weight: True\n",
      "model.encoder.layer.8.attention.self.value.bias: True\n",
      "model.encoder.layer.8.attention.output.dense.weight: True\n",
      "model.encoder.layer.8.attention.output.dense.bias: True\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.8.intermediate.dense.weight: True\n",
      "model.encoder.layer.8.intermediate.dense.bias: True\n",
      "model.encoder.layer.8.output.dense.weight: True\n",
      "model.encoder.layer.8.output.dense.bias: True\n",
      "model.encoder.layer.8.output.LayerNorm.weight: True\n",
      "model.encoder.layer.8.output.LayerNorm.bias: True\n",
      "model.encoder.layer.9.attention.self.query.weight: True\n",
      "model.encoder.layer.9.attention.self.query.bias: True\n",
      "model.encoder.layer.9.attention.self.key.weight: True\n",
      "model.encoder.layer.9.attention.self.key.bias: True\n",
      "model.encoder.layer.9.attention.self.value.weight: True\n",
      "model.encoder.layer.9.attention.self.value.bias: True\n",
      "model.encoder.layer.9.attention.output.dense.weight: True\n",
      "model.encoder.layer.9.attention.output.dense.bias: True\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.9.intermediate.dense.weight: True\n",
      "model.encoder.layer.9.intermediate.dense.bias: True\n",
      "model.encoder.layer.9.output.dense.weight: True\n",
      "model.encoder.layer.9.output.dense.bias: True\n",
      "model.encoder.layer.9.output.LayerNorm.weight: True\n",
      "model.encoder.layer.9.output.LayerNorm.bias: True\n",
      "model.encoder.layer.10.attention.self.query.weight: True\n",
      "model.encoder.layer.10.attention.self.query.bias: True\n",
      "model.encoder.layer.10.attention.self.key.weight: True\n",
      "model.encoder.layer.10.attention.self.key.bias: True\n",
      "model.encoder.layer.10.attention.self.value.weight: True\n",
      "model.encoder.layer.10.attention.self.value.bias: True\n",
      "model.encoder.layer.10.attention.output.dense.weight: True\n",
      "model.encoder.layer.10.attention.output.dense.bias: True\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.10.intermediate.dense.weight: True\n",
      "model.encoder.layer.10.intermediate.dense.bias: True\n",
      "model.encoder.layer.10.output.dense.weight: True\n",
      "model.encoder.layer.10.output.dense.bias: True\n",
      "model.encoder.layer.10.output.LayerNorm.weight: True\n",
      "model.encoder.layer.10.output.LayerNorm.bias: True\n",
      "model.encoder.layer.11.attention.self.query.weight: True\n",
      "model.encoder.layer.11.attention.self.query.bias: True\n",
      "model.encoder.layer.11.attention.self.key.weight: True\n",
      "model.encoder.layer.11.attention.self.key.bias: True\n",
      "model.encoder.layer.11.attention.self.value.weight: True\n",
      "model.encoder.layer.11.attention.self.value.bias: True\n",
      "model.encoder.layer.11.attention.output.dense.weight: True\n",
      "model.encoder.layer.11.attention.output.dense.bias: True\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.11.intermediate.dense.weight: True\n",
      "model.encoder.layer.11.intermediate.dense.bias: True\n",
      "model.encoder.layer.11.output.dense.weight: True\n",
      "model.encoder.layer.11.output.dense.bias: True\n",
      "model.encoder.layer.11.output.LayerNorm.weight: True\n",
      "model.encoder.layer.11.output.LayerNorm.bias: True\n",
      "classifier.linear_lvl1.weight: True\n",
      "classifier.linear_lvl1.bias: True\n",
      "classifier.linear_lvl2.weight: True\n",
      "classifier.linear_lvl2.bias: True\n",
      "classifier.linear_lvl3.weight: True\n",
      "classifier.linear_lvl3.bias: True\n",
      "classifier.softmax_reg1.weight: True\n",
      "classifier.softmax_reg1.bias: True\n",
      "classifier.softmax_reg2.weight: True\n",
      "classifier.softmax_reg2.bias: True\n",
      "classifier.softmax_reg3.weight: True\n",
      "classifier.softmax_reg3.bias: True\n"
     ]
    }
   ],
   "source": [
    "# Model class definition \n",
    "model_obj = BERT(\n",
    "    args_dict, num_labels=num_labels, dataset = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c458659",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, dev_set, test_set = model_obj.get_datasets()\n",
    "dataset = DatasetDict(\n",
    "{\n",
    "    \"train\": train_set,\n",
    "    \"valid\": dev_set,\n",
    "    \"test\": test_set\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26aa9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CUSTOM TRAINER: TrainerLossNetwork\n"
     ]
    }
   ],
   "source": [
    "trainer_class = Trainer\n",
    "if args_dict[\"custom_trainer\"] == \"TrainerLossNetwork\":\n",
    "   trainer_class = TrainerLossNetwork\n",
    "   print(\"USING CUSTOM TRAINER: TrainerLossNetwork\")\n",
    "if args_dict[\"custom_trainer\"] == \"TrainerDiceLoss\":\n",
    "   trainer_class = TrainerDiceLoss\n",
    "   print(\"USING CUSTOM TRAINER: TrainerDiceLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85cf28bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    filename,\n",
    "    evaluation_strategy=args_dict[\"evaluation_strategy\"],\n",
    "    eval_steps=args_dict[\"evaluation_steps\"],\n",
    "    logging_dir=filepath,\n",
    "    lr_scheduler_type=args_dict[\"lr_scheduler_type\"],\n",
    "    learning_rate=float(args_dict[\"lr_rate\"]),\n",
    "    warmup_ratio=args_dict[\"warm_up\"],\n",
    "    label_smoothing_factor=args_dict[\"label_smoothing\"],\n",
    "    per_device_train_batch_size=args_dict[\"batch_size\"],\n",
    "    per_device_eval_batch_size=args_dict[\"batch_size\"],\n",
    "    gradient_accumulation_steps=args_dict[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=args_dict[\"epochs\"],\n",
    "    weight_decay=args_dict[\"weight_decay\"],\n",
    "    logging_strategy=args_dict[\"logging_strategy\"],\n",
    "    logging_steps=args_dict[\"logging_steps\"],\n",
    "    load_best_model_at_end=args_dict[\"load_best\"],\n",
    "    metric_for_best_model=args_dict[\"metric_used\"],\n",
    "    greater_is_better=args_dict[\"greater_better\"],\n",
    "    save_strategy=args_dict[\"save_strategy\"],\n",
    "    save_steps=args_dict[\"save_steps\"],\n",
    "    save_total_limit=args_dict[\"save_limits\"],\n",
    "    dataloader_num_workers=args_dict[\"workers\"],\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=args_dict[\"drop_last\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69fe8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['checkpoint_torch_model']:\n",
    "    checkpoint = torch.load(args_dict['checkpoint_torch_model'], map_location='cuda')\n",
    "    model_obj.model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12edfe",
   "metadata": {},
   "source": [
    "## Train Model <a class=\"anchor\" id=\"Train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e55d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['task_type'] == 'flat-classification' or args_dict['task_type'] == 'NER':\n",
    "    evaluator = Metrics(dataset_test.features['label'].names).compute_metrics\n",
    "\n",
    "elif args_dict['task_type'] == 'hierarchical-classification':\n",
    "    decoder, normalized_decoder = model_obj.get_decoders()\n",
    "    evaluator = scorer.HierarchicalScorer(args_dict['experiment_name'], model_obj.get_tree(), normalized_decoder)\n",
    "    evaluator = evaluator.compute_metrics_transformers_hierarchy\n",
    "\n",
    "    assert (\n",
    "        all(len(elem)==3 for elem in dataset['train'].labels)\n",
    "    ), \"Something went wrong during encoding, all labels should have length of 3 (ignore if hierarchy level is not 3)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6568f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_class(\n",
    "    model_obj.model,\n",
    "    training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    tokenizer=model_obj.tokenizer,\n",
    "    compute_metrics=evaluator,\n",
    "    data_collator=data_collator,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience = 10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfca3ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [1, 5, 6],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [1, 5, 6],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [1, 4, 5],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 3, 4],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [3, 9, 10],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [4, 11, 12],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [1, 5, 6],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [1, 4, 5],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [4, 11, 12],\n",
       " [4, 10, 11],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [4, 10, 11],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 5, 6],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 4, 5],\n",
       " [4, 11, 12],\n",
       " [3, 9, 10],\n",
       " [4, 10, 11],\n",
       " [4, 11, 12],\n",
       " [3, 9, 10],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [4, 10, 11],\n",
       " [4, 11, 12],\n",
       " [3, 9, 10],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [2, 7, 8],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [1, 3, 4],\n",
       " [1, 4, 5],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [4, 11, 12],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 3, 4],\n",
       " [4, 10, 11],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 5, 6],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 5, 6],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 3, 4],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [1, 1, 1],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [1, 5, 6],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [1, 1, 1],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [1, 1, 1],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [2, 7, 8],\n",
       " [1, 5, 6],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [2, 7, 8],\n",
       " [4, 11, 12],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 2, 3],\n",
       " [4, 10, 11],\n",
       " [1, 4, 5],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [4, 10, 11],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [1, 5, 6],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [3, 9, 10],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [1, 3, 4],\n",
       " [4, 10, 11],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [4, 11, 12],\n",
       " [2, 7, 8],\n",
       " [4, 11, 12],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [4, 11, 12],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [1, 5, 6],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 4, 5],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [4, 11, 12],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 8, 9],\n",
       " [4, 10, 11],\n",
       " [1, 3, 4],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 5, 6],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [4, 11, 12],\n",
       " [4, 10, 11],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [1, 3, 4],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [4, 10, 11],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 2],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 3, 4],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 5, 6],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [2, 8, 9],\n",
       " [3, 9, 10],\n",
       " [3, 9, 10],\n",
       " [2, 7, 8],\n",
       " [1, 2, 3],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [4, 11, 12],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [4, 10, 11],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 4, 5],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 5, 6],\n",
       " [2, 6, 7],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [1, 5, 6],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [3, 9, 10],\n",
       " [2, 6, 7],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [3, 9, 10],\n",
       " [2, 8, 9],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [4, 11, 12],\n",
       " [2, 6, 7],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [2, 6, 7],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 3, 4],\n",
       " [1, 5, 6],\n",
       " [1, 2, 3],\n",
       " [2, 6, 7],\n",
       " [4, 10, 11],\n",
       " [2, 6, 7],\n",
       " [2, 6, 7],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [2, 7, 8],\n",
       " [4, 11, 12],\n",
       " [3, 9, 10],\n",
       " [1, 4, 5],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [4, 10, 11],\n",
       " [2, 8, 9],\n",
       " [1, 2, 3],\n",
       " [1, 1, 1],\n",
       " [2, 6, 7],\n",
       " [4, 11, 12],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 3, 4],\n",
       " [1, 1, 1],\n",
       " [4, 10, 11],\n",
       " [4, 10, 11],\n",
       " [1, 2, 3],\n",
       " [1, 5, 6],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 2, 3],\n",
       " [2, 8, 9],\n",
       " [2, 7, 8],\n",
       " [2, 8, 9],\n",
       " [1, 2, 2],\n",
       " [1, 3, 4],\n",
       " [2, 6, 7],\n",
       " [2, 7, 8],\n",
       " [1, 1, 1],\n",
       " [1, 4, 5],\n",
       " [1, 2, 2],\n",
       " [2, 7, 8],\n",
       " [1, 2, 2],\n",
       " [1, 2, 2],\n",
       " [1, 2, 3],\n",
       " [1, 4, 5],\n",
       " [1, 1, 1],\n",
       " [3, 9, 10],\n",
       " [1, 2, 2],\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].labels #should be normalized! cross entropy loss won't work otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d210f78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3292\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8240\n",
      "  1%|          | 100/8240 [00:22<24:48,  5.47it/s]***** Running Evaluation *****\n",
      "  Num examples = 366\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 56.4292, 'learning_rate': 6.067961165048544e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  1%|          | 100/8240 [02:10<24:48,  5.47it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-100\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-100\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 43.12958908081055, 'eval_h_f1': 0.22344139650872819, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.020969572098300932, 'eval_weighted_rec_lvl_3': 0.1448087431693989, 'eval_weighted_f1_lvl_3': 0.03663419278271189, 'eval_macro_prec_lvl_3': 0.012067395264116576, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.021081941129673824, 'eval_lvl_3_acc': 0.1448087431693989, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11369404879214072, 'eval_avg_weighted_rec': 0.2950819672131148, 'eval_avg_weighted_f1': 0.157649079660562, 'eval_avg_macro_prec': 0.05413423856046807, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.07504976231457496, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 107.9609, 'eval_samples_per_second': 3.39, 'eval_steps_per_second': 0.426, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-100\\special_tokens_map.json\n",
      "  2%|▏         | 200/8240 [02:31<25:56,  5.16it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 366\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 44.3161, 'learning_rate': 1.2135922330097088e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  2%|▏         | 200/8240 [02:37<25:56,  5.16it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 43.07456970214844, 'eval_h_f1': 0.22344139650872819, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.020969572098300932, 'eval_weighted_rec_lvl_3': 0.1448087431693989, 'eval_weighted_f1_lvl_3': 0.03663419278271189, 'eval_macro_prec_lvl_3': 0.012067395264116576, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.021081941129673824, 'eval_lvl_3_acc': 0.1448087431693989, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11369404879214072, 'eval_avg_weighted_rec': 0.2950819672131148, 'eval_avg_weighted_f1': 0.157649079660562, 'eval_avg_macro_prec': 0.05413423856046807, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.07504976231457496, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 5.364, 'eval_samples_per_second': 68.233, 'eval_steps_per_second': 8.576, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200\\special_tokens_map.json\n",
      "  4%|▎         | 300/8240 [02:57<22:57,  5.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 366\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 44.0122, 'learning_rate': 1.8203883495145632e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▎         | 300/8240 [03:03<22:57,  5.77it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 44.69132995605469, 'eval_h_f1': 0.09273947528981086, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.010779658992505003, 'eval_weighted_rec_lvl_3': 0.10382513661202186, 'eval_weighted_f1_lvl_3': 0.0195314613428556, 'eval_macro_prec_lvl_3': 0.008652094717668488, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.01567656765676568, 'eval_lvl_3_acc': 0.10382513661202186, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11029741109020874, 'eval_avg_weighted_rec': 0.28142076502732244, 'eval_avg_weighted_f1': 0.1519481691806099, 'eval_avg_macro_prec': 0.05299580504498538, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.0732479711569389, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 5.445, 'eval_samples_per_second': 67.218, 'eval_steps_per_second': 8.448, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300\\special_tokens_map.json\n",
      "  5%|▍         | 400/8240 [03:23<23:12,  5.63it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 366\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 44.7103, 'learning_rate': 2.4271844660194176e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  5%|▍         | 400/8240 [03:29<23:12,  5.63it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-400\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 44.64122772216797, 'eval_h_f1': 0.09273947528981086, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.010779658992505003, 'eval_weighted_rec_lvl_3': 0.10382513661202186, 'eval_weighted_f1_lvl_3': 0.0195314613428556, 'eval_macro_prec_lvl_3': 0.008652094717668488, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.01567656765676568, 'eval_lvl_3_acc': 0.10382513661202186, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11029741109020874, 'eval_avg_weighted_rec': 0.28142076502732244, 'eval_avg_weighted_f1': 0.1519481691806099, 'eval_avg_macro_prec': 0.05299580504498538, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.0732479711569389, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 5.501, 'eval_samples_per_second': 66.533, 'eval_steps_per_second': 8.362, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-400\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-200] due to args.save_total_limit\n",
      "  6%|▌         | 500/8240 [03:50<26:23,  4.89it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 366\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 32.6631, 'learning_rate': 3.0339805825242717e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  6%|▌         | 500/8240 [03:55<26:23,  4.89it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 43.00184631347656, 'eval_h_f1': 0.22344139650872819, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.020969572098300932, 'eval_weighted_rec_lvl_3': 0.1448087431693989, 'eval_weighted_f1_lvl_3': 0.03663419278271189, 'eval_macro_prec_lvl_3': 0.012067395264116576, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.021081941129673824, 'eval_lvl_3_acc': 0.1448087431693989, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11369404879214072, 'eval_avg_weighted_rec': 0.2950819672131148, 'eval_avg_weighted_f1': 0.157649079660562, 'eval_avg_macro_prec': 0.05413423856046807, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.07504976231457496, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 5.5132, 'eval_samples_per_second': 66.386, 'eval_steps_per_second': 8.344, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-300] due to args.save_total_limit\n",
      "  7%|▋         | 588/8240 [04:14<23:17,  5.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 42.2208, 'learning_rate': 4.2475728155339805e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  8%|▊         | 700/8240 [04:49<26:58,  4.66it/s]Saving model checkpoint to roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-700\n",
      "Configuration saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-700\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 44.59743881225586, 'eval_h_f1': 0.09273947528981086, 'eval_weighted_prec_lvl_1': 0.2723356923168802, 'eval_weighted_rec_lvl_1': 0.5218579234972678, 'eval_weighted_f1_lvl_1': 0.357898970872453, 'eval_macro_prec_lvl_1': 0.13046448087431695, 'eval_macro_rec_lvl_1': 0.25, 'eval_macro_f1_lvl_1': 0.17145421903052066, 'eval_lvl_1_acc': 0.5218579234972678, 'eval_lvl_1_matt_corr': 0.0, 'eval_weighted_prec_lvl_2': 0.047776881961241, 'eval_weighted_rec_lvl_2': 0.2185792349726776, 'eval_weighted_f1_lvl_2': 0.0784140753265211, 'eval_macro_prec_lvl_2': 0.01987083954297069, 'eval_macro_rec_lvl_2': 0.09090909090909091, 'eval_macro_f1_lvl_2': 0.03261312678353037, 'eval_lvl_2_acc': 0.2185792349726776, 'eval_lvl_2_matt_corr': 0.0, 'eval_weighted_prec_lvl_3': 0.010779658992505003, 'eval_weighted_rec_lvl_3': 0.10382513661202186, 'eval_weighted_f1_lvl_3': 0.0195314613428556, 'eval_macro_prec_lvl_3': 0.008652094717668488, 'eval_macro_rec_lvl_3': 0.08333333333333333, 'eval_macro_f1_lvl_3': 0.01567656765676568, 'eval_lvl_3_acc': 0.10382513661202186, 'eval_lvl_3_matt_corr': 0.0, 'eval_avg_weighted_prec': 0.11029741109020874, 'eval_avg_weighted_rec': 0.28142076502732244, 'eval_avg_weighted_f1': 0.1519481691806099, 'eval_avg_macro_prec': 0.05299580504498538, 'eval_avg_macro_rec': 0.1414141414141414, 'eval_avg_macro_f1': 0.0732479711569389, 'eval_avg_acc': 0.0, 'eval_avg_matt_corr': 0.0, 'eval_runtime': 5.5813, 'eval_samples_per_second': 65.576, 'eval_steps_per_second': 8.242, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-700\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-700\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\checkpoint-500] due to args.save_total_limit\n",
      " 10%|▉         | 800/8240 [05:10<24:08,  5.14it/s]  Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorboard\\summary\\writer\\record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 766, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 160, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"C:\\Users\\mandy\\anaconda3\\envs\\gpu2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 164, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'log\\\\hierarchical-classification\\\\roberta\\\\roberta-hierarchical_FL8_E20_B8_LR5e-05_WD2_V003\\\\events.out.tfevents.1647265432.DANYI-G14.14808.1'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=args_dict['resume_from_checkpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to saving from here\n",
    "Path(f\"{filename}/models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{filename}/torch_pretrained\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_collector = ResultCollector(args_dict['data_file'], filename)  \n",
    "for split in ['train', 'valid', 'test']:\n",
    "    result_collector.results['{}+{}'.format(args_dict['experiment_name'], split)] \\\n",
    "        = trainer.evaluate(dataset[split])\n",
    "result_collector.persist_results(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['task_type'] == 'flat-classification' or args_dict['task_type'] == 'NER':\n",
    "     logits, labels, metrics = trainer.predict(dataset[\"test\"])\n",
    "     predictions = logits.argmax(1)\n",
    "     \n",
    "     report = classification_report(\n",
    "        labels, \n",
    "        predictions,\n",
    "        target_names= dataset['test'].features['label'].names #target_names\n",
    "     )\n",
    "     print(report)     \n",
    "     with open(f'{filename}/results/metrics.json', 'w') as metrics:\n",
    "          json.dump(report, metrics, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3396357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['task_type'] == 'hierarchical-classification':\n",
    "     prediction = trainer.predict(dataset['test'])\n",
    "     preds =  np.array(np.array([list(pred.argmax(-1)) for pred in prediction.predictions]).transpose().tolist())\n",
    "\n",
    "     #TODO: hardcoded for 3 levels\n",
    "     #labels + prediction are normalized, to get original path/label name use normalized_decoder first and then the decoder (just like below)\n",
    "\n",
    "     label_list = []\n",
    "     predcition_list = []\n",
    "     for i in range(3):\n",
    "          # normalized decoder: from derived_key to original key; decoder: from original_key to label name\n",
    "          label_list.append([decoder[i+1][normalized_decoder[i+1][label]]['name'] for label in prediction.label_ids[:, i]])\n",
    "          predcition_list.append([decoder[i+1][normalized_decoder[i+1][prediction]]['name'] for prediction in preds[:, i]])\n",
    "\n",
    "     test_pred=pd.DataFrame(data={\n",
    "     \"label_lvl1\": label_list[0] ,\"prediction_lvl1\": predcition_list[0], \n",
    "     \"label_lvl2\": label_list[1] ,\"prediction_lvl2\": predcition_list[1],\n",
    "     \"label_lvl3\": label_list[2] ,\"prediction_lvl3\": predcition_list[2]\n",
    "     }) \n",
    "\n",
    "     assert (\n",
    "     len(dataset['test']) == len(test_pred)\n",
    "     ), \"Something went wrong, length of test datasets should be the same\"\n",
    "\n",
    "     full_prediction_output = '{}/{}.csv'.format(f\"{filename}/results\", \"prediction-results\")\n",
    "     test_pred.to_csv(full_prediction_output, index=False, sep=';', encoding='utf-8', quotechar='\"',\n",
    "          quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl = 1\n",
    "for label, prediction in zip(label_list, predcition_list):\n",
    "    np.save(Path(f'{filename}/results/').joinpath(f\"confusion_lvl{lvl}.npy\"), confusion_matrix(label, prediction))\n",
    "    lvl += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fec001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results hierarchy level 1\n",
    "log1 = classification_report(\n",
    "        label_list[0], \n",
    "        predcition_list[0],\n",
    "        output_dict = True\n",
    "    )\n",
    "#print(log1)\n",
    "with open(f'{filename}/results/classification_report1.json', 'w') as metrics:\n",
    "    json.dump(log1, metrics, indent=4)\n",
    "\n",
    "label = set(label_list[0])\n",
    "array = np.load(Path(f'{filename}/results').joinpath(\"confusion_lvl1.npy\"))\n",
    "\n",
    "df_cm = pd.DataFrame(array, index = label,\n",
    "                  columns = label)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4309c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results hierarchy level 2\n",
    "log2 = classification_report(\n",
    "        label_list[1], \n",
    "        predcition_list[1],\n",
    "        output_dict = True\n",
    "    )\n",
    "#print(log2)\n",
    "with open(f'{filename}/results/classification_report2.json', 'w') as metrics:\n",
    "    json.dump(log2, metrics, indent=4)\n",
    "\n",
    "label = set(label_list[1])\n",
    "array = np.load(Path(f'{filename}/results/').joinpath(\"confusion_lvl2.npy\"))\n",
    "\n",
    "df_cm = pd.DataFrame(array, index = label,\n",
    "                  columns = label)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8337796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results hierarchy level 3\n",
    "log3 = classification_report(\n",
    "        label_list[2], \n",
    "        predcition_list[2],\n",
    "        output_dict = True\n",
    "    )\n",
    "#print(log3)\n",
    "with open(f'{filename}/results/classification_report3.json', 'w') as metrics:\n",
    "    json.dump(log3, metrics, indent=4)\n",
    "\n",
    "label = set(label_list[2])\n",
    "array = np.load(Path(f'{filename}/results/').joinpath(\"confusion_lvl3.npy\"))\n",
    "\n",
    "df_cm = pd.DataFrame(array, index = label,\n",
    "                  columns = label)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done... saving model\")\n",
    "\n",
    "trainer.save_model(f\"{filename}/models\")\n",
    "model_obj.model.save_pretrained(f\"{filename}/pretrained\")\n",
    "model_obj.tokenizer.save_pretrained(f\"{filename}/pretrained\")\n",
    "model_obj.model.save(model_obj.model, trainer.optimizer, f\"{filename}/torch_pretrained/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CLEAN UP CODE, a lot is still hard coded for 3 hierarchy levels"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501d3dedde2a9b7d8234feb6533e46662b5732052cc258c64081b7e508575ff"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('gpu2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
