{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26b94ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# MAKI4U Jumpstart Notebook\n",
    "\n",
    "A Notebook for training new BERT models for MAKI4U (former CCAI)\\\n",
    "This is a refactored version of \"bert_train_classifier.ipynb\" from the\n",
    "BAS Jumpstart\\ and is meant as optimization and general clean up of that notebook\\\n",
    "It is possible to use this as notebook or directly as a script\n",
    "\n",
    "\n",
    "This notebook is organized in\n",
    "* [Configuration for Model and Logging](#config)\n",
    "* [Loading Dataset](#dataset)\n",
    "* [Model Definition](#model)\n",
    "* [Train Model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef827764",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc326a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython import get_ipython\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "import yaml\n",
    "from utils.bert_custom_trainer import TrainerDiceLoss\n",
    "from utils.configuration import (\n",
    "    parse_arguments,\n",
    "    save_config,\n",
    "    yaml_dump_for_notebook,\n",
    "    isnotebook,\n",
    ")\n",
    "from utils.metrics import compute_metrics\n",
    "from utils import scorer\n",
    "from utils.BERT import BERT\n",
    "from utils.result_collector import ResultCollector\n",
    "from typing import List, Set, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.9.0.dev0\")\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5bb6f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62649c",
   "metadata": {},
   "source": [
    "## Configuration and Logging <a class=\"anchor\" id=\"config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra variable for processing big files in BERT\n",
    "# https://github.com/huggingface/datasets/issues/2181\n",
    "block_size_10MB = 10 << 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7139d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if isnotebook():\n",
    "    # In a notebook you need to just dump the yaml with the configuration details\n",
    "args_dict = yaml_dump_for_notebook(filepath='configs/hierarchical-baseline.yml')\n",
    "#print(args_dict)\n",
    "#else:\n",
    "#    # This can only be used if this is run as a script. For notebooks use the yaml.dump and configure the yaml file accordingly\n",
    "#    args, device = parse_arguments()\n",
    "#    args_dict = args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename, filepath = save_config(args_dict)\n",
    "print(filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95baf424",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"{filepath}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb4d72",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Loading Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "### TODO: Refactor this properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab68622",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "json_files = str(Path(args_dict[\"data_folder\"]).joinpath(args_dict[\"data_file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefd261",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "json_files_train = [json_files.replace(\".json\", \"\") + \"_train2.json\"]\n",
    "json_files_test = [json_files.replace(\".json\", \"\") + \"_test.json\"]\n",
    "\n",
    "dataset_train = load_dataset(\n",
    "    \"json\", data_files=json_files_train, chunksize=block_size_10MB\n",
    ")[\"train\"]\n",
    "\n",
    "dataset_test = load_dataset(\n",
    "    \"json\", data_files=json_files_test, chunksize=block_size_10MB\n",
    ")[\"train\"]\n",
    "\n",
    "#dataset_train = dataset_train.class_encode_column(\"label\")\n",
    "#dataset_test = dataset_test.class_encode_column(\"label\")\n",
    "\n",
    "# assert (\n",
    "#     dataset_train.features[\"label\"].names == dataset_test.features[\"label\"].names # == dataset_dev.features[\"label\"].names\n",
    "# ), \"Something went wrong, target_names of train and test should be the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args_dict[\"checkpoint_tokenizer\"], use_fast=True, model_max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True), \n",
    "    batched=True\n",
    "    #num_proc=args_dict[\"tokenizer_num_processes\"]\n",
    ")\n",
    "\n",
    "#dataset_dev = dataset_dev.map(\n",
    "#    lambda x: tokenizer(x['text'], truncation=True), \n",
    "#    batched=True\n",
    "#    #num_proc=args_dict[\"tokenizer_num_processes\"]\n",
    "#)\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True), \n",
    "    batched=True\n",
    "    #num_proc=args_dict[\"tokenizer_num_processes\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(np.unique(dataset_train['label'])) #dataset_train.features[\"label\"].num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shuffle to make sure the order of samples is randomized (deterministically)\n",
    "dataset_train = dataset_train.shuffle(seed=args_dict[\"random_seed\"])\n",
    "#dataset_dev = dataset_dev.shuffle(seed=args_dict[\"random_seed\"])\n",
    "ds_train_testvalid = dataset_train.train_test_split(\n",
    "    test_size=(1 - args_dict[\"split_ratio_train\"])\n",
    ")\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train_testvalid['train'],\n",
    "        \"valid\": ds_train_testvalid['test'],\n",
    "        \"test\": dataset_test\n",
    "    }\n",
    ")\n",
    "\n",
    "target_names = np.unique(dataset[\"test\"][\"label\"])\n",
    "\n",
    "if args_dict[\"oversampling\"]:\n",
    "    df_train = dataset[\"train\"].to_pandas()\n",
    "    min_samples = math.ceil(len(df_train) * args_dict[\"oversampling\"])\n",
    "    count_dict = dict(df_train[\"label\"].value_counts())\n",
    "    count_dict = {k: v for k, v in count_dict.items() if v < min_samples}\n",
    "\n",
    "    over_samples = []\n",
    "    for label_id, n_occurance in count_dict.items():\n",
    "        class_samples = df_train[df_train[\"label\"] == label_id]\n",
    "        additional_samples = class_samples.sample(\n",
    "            n=(min_samples - len(class_samples)), replace=True\n",
    "        )\n",
    "        over_samples.append(additional_samples)\n",
    "        print(\n",
    "            f\"\\nAdding {len(additional_samples)} samples for class {target_names[label_id]}\"\n",
    "        )\n",
    "\n",
    "    new_train = pd.concat([df_train, *over_samples])\n",
    "    dataset[\"train\"] = Dataset.from_pandas(new_train)\n",
    "\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args_dict[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807e2ab",
   "metadata": {},
   "source": [
    "## Model Definition <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66ead9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Model class definition was moved to utils for easier mentainance across notebooks\n",
    "model_obj = BERT(\n",
    "    args_dict, num_labels=num_labels, dataset = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d225e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = model_obj.get_decoder()\n",
    "train_set, dev_set, test_set = model_obj.get_datasets()\n",
    "dataset = DatasetDict(\n",
    "   {\n",
    "       \"train\": train_set,\n",
    "       \"valid\": dev_set,\n",
    "       \"test\": test_set\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8976d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_class = Trainer\n",
    "#if args_dict[\"custom_trainer\"] == \"TrainerDiceLoss\":\n",
    "#    trainer_class = TrainerDiceLoss\n",
    "#    print(\"USING CUSTOM TRAINER CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ed1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    filename,\n",
    "    evaluation_strategy=args_dict[\"evaluation_strategy\"],\n",
    "    eval_steps=args_dict[\"evaluation_steps\"],\n",
    "    logging_dir=filepath,\n",
    "    lr_scheduler_type=args_dict[\"lr_scheduler_type\"],\n",
    "    learning_rate=float(args_dict[\"lr_rate\"]),\n",
    "    warmup_ratio=args_dict[\"warm_up\"],\n",
    "    label_smoothing_factor=args_dict[\"label_smoothing\"],\n",
    "    per_device_train_batch_size=args_dict[\"batch_size\"],\n",
    "    per_device_eval_batch_size=args_dict[\"batch_size\"],\n",
    "    gradient_accumulation_steps=args_dict[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=args_dict[\"epochs\"],\n",
    "    weight_decay=args_dict[\"weight_decay\"],\n",
    "    logging_strategy=args_dict[\"logging_strategy\"],\n",
    "    logging_steps=args_dict[\"logging_steps\"],\n",
    "    load_best_model_at_end=args_dict[\"load_best\"],\n",
    "    metric_for_best_model=args_dict[\"metric_used\"],\n",
    "    greater_is_better=args_dict[\"greater_better\"],\n",
    "    save_strategy=args_dict[\"save_strategy\"],\n",
    "    save_steps=args_dict[\"save_steps\"],\n",
    "    save_total_limit=args_dict[\"save_limits\"],\n",
    "    dataloader_num_workers=args_dict[\"workers\"],\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=args_dict[\"load_best\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2e7e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Train Model <a class=\"anchor\" id=\"Train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ccd32b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mandy\\Documents\\transformer-project\\ccai-bert-train\\Classification.ipynb Cell 25'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=0'>1</a>\u001b[0m evaluator \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39mHierarchicalScorer(args_dict[\u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m], model_obj\u001b[39m.\u001b[39mget_tree(), decoder\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=1'>2</a>\u001b[0m             )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m trainer_class(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=3'>4</a>\u001b[0m     model_obj\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=4'>5</a>\u001b[0m     training_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=9'>10</a>\u001b[0m     data_collator \u001b[39m=\u001b[39m data_collator\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=10'>11</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=12'>13</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49margs_dict[\u001b[39m\"\u001b[39;49m\u001b[39mresume_from_checkpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/Classification.ipynb#ch0000024?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinished training\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1332\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1329'>1330</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1330'>1331</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1331'>1332</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1333'>1334</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1334'>1335</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1335'>1336</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1336'>1337</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1337'>1338</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1338'>1339</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1339'>1340</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1891\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1887'>1888</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1889'>1890</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1890'>1891</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1892'>1893</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1893'>1894</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1923\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1920'>1921</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1921'>1922</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1922'>1923</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1923'>1924</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1924'>1925</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/transformers/trainer.py?line=1925'>1926</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mandy\\Documents\\transformer-project\\ccai-bert-train\\HierarchicalClassificationHead.py:61\u001b[0m, in \u001b[0;36mClassificationModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=58'>59</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=60'>61</a>\u001b[0m     logits, loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(sequence_output, labels)\n\u001b[0;32m     <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m     <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=63'>64</a>\u001b[0m     output \u001b[39m=\u001b[39m (logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mandy\\Documents\\transformer-project\\ccai-bert-train\\HierarchicalClassificationHead.py:148\u001b[0m, in \u001b[0;36mHierarchicalClassificationHead.forward\u001b[1;34m(self, input, labels)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=144'>145</a>\u001b[0m logit_list \u001b[39m=\u001b[39m [node(\u001b[39minput\u001b[39m) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes[lvl]]\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=145'>146</a>\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(logit_list, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=147'>148</a>\u001b[0m updated_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_label_per_lvl(labels, lvl, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=149'>150</a>\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=150'>151</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels_per_lvl[lvl]), updated_labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\mandy\\Documents\\transformer-project\\ccai-bert-train\\HierarchicalClassificationHead.py:228\u001b[0m, in \u001b[0;36mHierarchicalClassificationHead.update_label_per_lvl\u001b[1;34m(self, labels, lvl, all_paths)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=225'>226</a>\u001b[0m updated_labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mclone()\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=226'>227</a>\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m unique_values:\n\u001b[1;32m--> <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=227'>228</a>\u001b[0m     searched_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpaths_per_lvl[\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpaths_per_lvl)][value]\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=228'>229</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(searched_path) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m lvl:\n\u001b[0;32m    <a href='file:///c%3A/Users/mandy/Documents/transformer-project/ccai-bert-train/HierarchicalClassificationHead.py?line=229'>230</a>\u001b[0m         update_value \u001b[39m=\u001b[39m searched_path[lvl \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluator = scorer.HierarchicalScorer(args_dict['experiment_name'], model_obj.get_tree(), decoder\n",
    "            )\n",
    "trainer = trainer_class(\n",
    "    model_obj.model,\n",
    "    training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    tokenizer=model_obj.tokenizer,\n",
    "    compute_metrics=evaluator.compute_metrics_transformers_flat,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=args_dict[\"resume_from_checkpoint\"])\n",
    "\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = trainer.evaluate()\n",
    "print(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57886731-b0c1-4547-9540-4a42a36637e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_collector = ResultCollector(args_dict['data_file'].split(\".\")[0], args_dict['experiment_name'])  \n",
    "for split in ['train', 'valid', 'test']:\n",
    "    result_collector.results['{}+{}'.format(args_dict['experiment_name'], split)] \\\n",
    "        = trainer.evaluate(dataset[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to saving from here\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(f\"models/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(dataset['test'])\n",
    "preds = prediction.predictions.argmax(-1)\n",
    "\n",
    "dataset['test']['prediction'] = [decoder[pred]['value'] for pred in preds]\n",
    "full_prediction_output = '{}/{}.csv'.format(f\"models/{filename}\", \"prediction-results\")\n",
    "\n",
    "dataset['test'].to_csv(full_prediction_output, index=False, sep=';', encoding='utf-8', quotechar='\"',\n",
    "                quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Persist results\n",
    "result_collector.persist_results(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb755948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits, labels, metrics = trainer.predict(dataset[\"test\"])\n",
    "#predictions = logits.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Log this properly\n",
    "\n",
    "log = classification_report(\n",
    "        prediction.labels, \n",
    "        preds, \n",
    "        #np.unique(labels),\n",
    "        target_names= np.unique(dataset_test['label']) #dataset[\"test\"].features[\"label\"].names \n",
    "    )\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done... saving model\")\n",
    "\n",
    "trainer.save_model(f\"models/{filename}\")\n",
    "#model_obj.model.save_pretrained(f\"pretrained/{filename}\")\n",
    "model_obj.tokenizer.save_pretrained(f\"pretrained/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4741b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open(f\"pretrained/{filename}/classification_report.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(log)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501d3dedde2a9b7d8234feb6533e46662b5732052cc258c64081b7e508575ff"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
