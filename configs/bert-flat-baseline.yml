# This file is used as a baseline for Hierarchical Classification
# These are all the possible arguments to configure the trainer
# Please changes the ones you want to change
# and delete all options you do not want to call explicity

# The name of the task you are training
task_type: "flat-classification"
# GPU(cuda) or CPU
device: "cuda"
# an identifyable name (suffix)
experiment_name: "dbmdz-flat"
experiment_name_suffix: null
# folder and file data is stored in
data_folder: "./data/blurbs_dataset"
data_file: "blurbs_reduced_full.json"
data_lvl: 1
# Random Seed to generate reproducable random things
random_seed: 42

# The architecture of the model you're using
architecture: "bert"
# Resume training
resume_from_checkpoint: null
# the checkpoint model to work from
checkpoint_torch_model: False 
checkpoint_model_or_path: "dbmdz/bert-base-german-cased"
#
tokenizer_num_processes: 10
# Number of Workers
workers: 2
# load the best model after training
load_best: True
# is a higher value better in the metric you use?
greater_better: True
# metric you want to use
metric_used: "matthews_correlation" 
drop_last: False

#try to get batch_size as high as possible
batch_size: 32
gradient_accumulation_steps: 1
# Learning Rate, 1e-05
lr_rate: 8e-05
#5.0e-1 = 0.5
weight_decay: 0.005
# Warm up for the learning rate
warm_up: 0.1
lr_scheduler_type: "linear"
epochs: 30
oversampling: 0.1
# Number of layers in the pretrained model to not train
freeze_layer: 11
label_smoothing: 0
hidden_dropout_prob: 0.1 #default: 0.1
# Custom Trainer
custom_trainer: "TrainerDiceLoss"
# Split Ratio
split_ratio_train: 0.7

# Should there be a tensorboard log?
logging: True
logging_strategy: "steps"
#same as evaluatian steps
logging_steps: 50
evaluation_strategy: "steps"
evaluation_steps: 100
save_strategy: "steps"
save_steps: 1000
save_limits: 3