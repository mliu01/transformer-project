# This file is used as a baseline
# These are all the possible arguments to configure the trainer
# Please changes the ones you want to change
# and delete all options you do not want to call explicity

# The name of the task you are training
task_type: "hierarchical-classification"
# GPU(cuda) or CPU
device: "cuda"
# an identifyable name (suffix)
experiment_name: "roberta-hierarchical"
experiment_name_suffix: "freeze-10"
# folder and file data is stored in
data_folder: "./data"
data_file: "part-blurbs"
# Random Seed to generate reproducable random things
random_seed: 42

# The architecture of the model you're using
architecture: "roberta"
# Resume training
resume_from_checkpoint: False
# the checkpoint model to work from
checkpoint_torch_model: False #"roberta-hierarchical_freeze-10_E5_B16_LR3e-04_WD0.5_V008/torch_pretrained/model.pth"
checkpoint_model: "uklfr/gottbert-base" #"roberta-hierarchical_freeze-10_E50_B16_LR3e-04_WD0.5_V001/pretrained"
# the tokenizer checkpoint to work from
checkpoint_tokenizer: "uklfr/gottbert-base" #"roberta-hierarchical_freeze-10_E50_B16_LR3e-04_WD0.5_V001/pretrained"
#
tokenizer_num_processes: 10
# Number of Workers
workers: 2
# load the best model after training
load_best: True
# is a higher value better in the metric you use?
greater_better: True
# metric you want to use
metric_used: "h_f1"
drop_last: False

#try to get batch_size as high as possible
batch_size: 16
gradient_accumulation_steps: 1
# Learning Rate, 1e-05
lr_rate: 1e-04
#5.0e-1 = 0.5
weight_decay: 5.0e-1
# Warm up for the learning rate
warm_up: 0.2
lr_scheduler_type: "linear"
epochs: 10
oversampling: 0
# Number of layers in the pretrained model to not train
freeze_layer: 10
label_smoothing: 0
# Custom Trainer
custom_trainer: None #"TrainerDiceLoss"
# Split Ratio
split_ratio_train: 0.75

# Should there be a tensorboard log?
logging: True
logging_strategy: "steps"
#same as evaluatian steps
logging_steps: 1000
evaluation_strategy: "steps"
evaluation_steps: 1000
save_strategy: "steps"
save_steps: 2000
save_limits: 5
