# This file is used as a baseline for Hierarchical Classification
# These are all the possible arguments to configure the trainer
# Please changes the ones you want to change
# and delete all options you do not want to call explicity

# The name of the task you are training
task_type: "lcpn-hierarchical-classification"
# GPU(cuda) or CPU
device: "cuda"
# an identifyable name (suffix)
experiment_name: "roberta-lcpn"
experiment_name_suffix: null
# folder and file data is stored in
data_folder: "./data/blurbs_dataset"
data_file: "blurbs_reduced_full.json"
# Random Seed to generate reproducable random things
random_seed: 42

# The architecture of the model you're using
architecture: "roberta"
# Resume training
resume_from_checkpoint: null
# the checkpoint model to work from
checkpoint_torch_model: False 
checkpoint_model_or_path: "uklfr/gottbert-base"
#
tokenizer_num_processes: 10
# Number of Workers
workers: 0 # set to 0 when debugging
# load the best model after training
load_best: True
# is a higher value better in the metric you use?
greater_better: True
# metric you want to use
metric_used: "h_f1" 
drop_last: False

#try to get batch_size as high as possible
batch_size: 8
gradient_accumulation_steps: 1
# Learning Rate, 1e-05
lr_rate: 8e-05
#5.0e-1 = 0.5
weight_decay: 0.005
# Warm up for the learning rate
warm_up: 0.1
lr_scheduler_type: "linear"
epochs: 5
oversampling: 0.05
# Number of layers in the pretrained model to not train
freeze_layer: 8
label_smoothing: 0
hidden_dropout_prob: 0.1 #default: 0.1
attention_probs_dropout_prob: 0.5
# Custom Trainer
custom_trainer: None 
# Split Ratio
split_ratio_train: 0.7

# Should there be a tensorboard log?
logging: True
logging_strategy: "steps"
#same as evaluatian steps
logging_steps: 200
evaluation_strategy: "steps"
evaluation_steps: 500
save_strategy: "steps"
save_steps: 5000
save_limits: 3